<Type Name="FeatureContributionCalculatingTransformer" FullName="Microsoft.ML.Transforms.FeatureContributionCalculatingTransformer">
  <TypeSignature Language="C#" Value="public sealed class FeatureContributionCalculatingTransformer : Microsoft.ML.Data.OneToOneTransformerBase" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi sealed beforefieldinit FeatureContributionCalculatingTransformer extends Microsoft.ML.Data.OneToOneTransformerBase" />
  <TypeSignature Language="DocId" Value="T:Microsoft.ML.Transforms.FeatureContributionCalculatingTransformer" />
  <TypeSignature Language="VB.NET" Value="Public NotInheritable Class FeatureContributionCalculatingTransformer&#xA;Inherits OneToOneTransformerBase" />
  <TypeSignature Language="F#" Value="type FeatureContributionCalculatingTransformer = class&#xA;    inherit OneToOneTransformerBase" />
  <AssemblyInfo>
    <AssemblyName>Microsoft.ML.Data</AssemblyName>
    <AssemblyVersion>1.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>Microsoft.ML.Data.OneToOneTransformerBase</BaseTypeName>
  </Base>
  <Interfaces />
  <Docs>
    <summary>
            The FeatureContributionCalculationTransformer computes model-specific per-feature contributions to the score of each example.
            See the list of currently supported models below.
            </summary>
    <remarks>
      <para>
            Scoring a dataset with a trained model produces a score, or prediction, for each example. To understand and explain these predictions
            it can be useful to inspect which features influenced them most significantly. FeatureContributionCalculationTransformer computes a model-specific
            list of per-feature contributions to the score for each example. These contributions can be positive (they make the score higher) or negative
            (they make the score lower).
            </para>
      <para>
            Feature Contribution Calculation is currently supported for the following models:
                Regression:
                    OrdinaryLeastSquares, StochasticDualCoordinateAscent (SDCA), OnlineGradientDescent, PoissonRegression,
                    GeneralizedAdditiveModels (GAM), LightGbm, FastTree, FastForest, FastTreeTweedie
                Binary Classification:
                    AveragedPerceptron, LinearSupportVectorMachines, LogisticRegression, StochasticDualCoordinateAscent (SDCA),
                    StochasticGradientDescent (SGD), SymbolicStochasticGradientDescent, GeneralizedAdditiveModels (GAM),
                    FastForest, FastTree, LightGbm
                Ranking:
                    FastTree, LightGbm
            </para>
      <para>
            For linear models, the contribution of a given feature is equal to the product of feature value times the corresponding weight. Similarly,
            for Generalized Additive Models (GAM), the contribution of a feature is equal to the shape function for the given feature evaluated at
            the feature value.
            </para>
      <para>
            For tree-based models, the calculation of feature contribution essentially consists in determining which splits in the tree have the most impact
            on the final score and assigning the value of the impact to the features determining the split. More precisely, the contribution of a feature
            is equal to the change in score produced by exploring the opposite sub-tree every time a decision node for the given feature is encountered.
            Consider a simple case with a single decision tree that has a decision node for the binary feature F1. Given an example that has feature F1
            equal to true, we can calculate the score it would have obtained if we chose the subtree corresponding to the feature F1 being equal to false
            while keeping the other features constant. The contribution of feature F1 for the given example is the difference between the original score
            and the score obtained by taking the opposite decision at the node corresponding to feature F1. This algorithm extends naturally to models with
            many decision trees.
            </para>
      <para>
            See the sample below for an example of how to compute feature importance using the FeatureContributionCalculatingTransformer.
            </para>
    </remarks>
  </Docs>
  <Members />
</Type>