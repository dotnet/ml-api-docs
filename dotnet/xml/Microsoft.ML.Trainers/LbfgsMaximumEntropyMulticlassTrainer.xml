<Type Name="LbfgsMaximumEntropyMulticlassTrainer" FullName="Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer">
  <TypeSignature Language="C#" Value="public sealed class LbfgsMaximumEntropyMulticlassTrainer : Microsoft.ML.Trainers.LbfgsTrainerBase&lt;Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer.Options,Microsoft.ML.Data.MulticlassPredictionTransformer&lt;Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;,Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi sealed beforefieldinit LbfgsMaximumEntropyMulticlassTrainer extends Microsoft.ML.Trainers.LbfgsTrainerBase`3&lt;class Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer/Options, class Microsoft.ML.Data.MulticlassPredictionTransformer`1&lt;class Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;, class Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;" />
  <TypeSignature Language="DocId" Value="T:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer" />
  <TypeSignature Language="VB.NET" Value="Public NotInheritable Class LbfgsMaximumEntropyMulticlassTrainer&#xA;Inherits LbfgsTrainerBase(Of LbfgsMaximumEntropyMulticlassTrainer.Options, MulticlassPredictionTransformer(Of MaximumEntropyModelParameters), MaximumEntropyModelParameters)" />
  <TypeSignature Language="F#" Value="type LbfgsMaximumEntropyMulticlassTrainer = class&#xA;    inherit LbfgsTrainerBase&lt;LbfgsMaximumEntropyMulticlassTrainer.Options, MulticlassPredictionTransformer&lt;MaximumEntropyModelParameters&gt;, MaximumEntropyModelParameters&gt;" />
  <AssemblyInfo>
    <AssemblyName>Microsoft.ML.StandardTrainers</AssemblyName>
    <AssemblyVersion>1.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>Microsoft.ML.Trainers.LbfgsTrainerBase&lt;Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer+Options,Microsoft.ML.Data.MulticlassPredictionTransformer&lt;Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;,Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;</BaseTypeName>
    <BaseTypeArguments>
      <BaseTypeArgument TypeParamName="TOptions">Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer+Options</BaseTypeArgument>
      <BaseTypeArgument TypeParamName="TTransformer">Microsoft.ML.Data.MulticlassPredictionTransformer&lt;Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;</BaseTypeArgument>
      <BaseTypeArgument TypeParamName="TModel">Microsoft.ML.Trainers.MaximumEntropyModelParameters</BaseTypeArgument>
    </BaseTypeArguments>
  </Base>
  <Interfaces />
  <Docs>
    <summary>
             The <see cref="T:Microsoft.ML.IEstimator`1" /> to predict a target using a maximum entropy multiclass classifier trained with L-BFGS method.
             </summary>
    <remarks>
      <format type="text/markdown"><![CDATA[
             To create this trainer, use [LbfgsMaximumEntropy](xref:Microsoft.ML.StandardTrainersCatalog.LbfgsMaximumEntropy(Microsoft.ML.MulticlassClassificationCatalog.MulticlassClassificationTrainers,System.String,System.String,System.String,System.Single,System.Single,System.Single,System.Int32,System.Boolean))
             or [LbfgsMaximumEntropy(Options)](xref:Microsoft.ML.StandardTrainersCatalog.LbfgsMaximumEntropy(Microsoft.ML.MulticlassClassificationCatalog.MulticlassClassificationTrainers,Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer.Options)).
            
             [!include[io](~/../docs/samples/docs/api-reference/io-columns-multiclass-classification.md)]
            
             ### Trainer Characteristics
             |  |  |
             | -- | -- |
             | Machine learning task | Multiclass classification |
             | Is normalization required? | Yes |
             | Is caching required? | No |
             | Required NuGet in addition to Microsoft.ML | None |
            
             ### Scoring Function
             [Maximum entropy model](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) is a generalization of linear [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression).
             The major difference between maximum entropy model and logistic regression is that the number of classes supported in considered classification problem.
             Logistic regression is only for binary classification while maximum entropy model handles multiple classes.
             See Section 1 in [this paper](https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf) for a detailed introduction.
            
             Assume that the number of classes is $m$ and number of features is $n$.
             Maximum entropy model assigns the $c$-th class a coefficient vector $\textbf \emph x_c \in {\mathbb R}^n$ and a bias $b_c \in {\mathbb R}$, for $c=1,\dots,m$.
             Given a feature vector $\textbf x \in {\mathbb R}^n$, the $c$-th class's score is $\hat{y}^c = \textbf w_c^T \textbf x + b_c$.
             The probability of $\textbf x$ belonging to class $c$ is defined by $\tilde{P}(c|\textbf x) = \frac{ e^{\hat{y}^c} }{ \sum_{c' = 1}^m e^{\hat{y}^{c'}} }$.
             Let $P(c, \textbf x)$ denote the join probability of seeing $c$ and $\textbf x$.
             The loss function minimized by this trainer is $-\sum_{c = 1}^m P(c, \textbf x) \log \tilde{P}(c|\textbf x) $, which is the negative [log-likelihood function](https://en.wikipedia.org/wiki/Likelihood_function#Log-likelihood).
            
             ### Training Algorithm Details
             The optimization technique implemented is based on [the limited memory Broyden-Fletcher-Goldfarb-Shanno method (L-BFGS)](https://en.wikipedia.org/wiki/Limited-memory_BFGS).
             L-BFGS is a [quasi-Newtonian method](https://en.wikipedia.org/wiki/Quasi-Newton_method) which replaces the expensive computation cost of Hessian matrix with an approximation but still enjoys a fast convergence rate like [Newton method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) where the full Hessian matrix is computed.
             Since L-BFGS approximation uses only a limited amount of historical states to compute the next step direction, it is especially suited for problems with high-dimensional feature vector.
             The number of historical states is a user-specified parameter, using a larger number may lead to a better approximation to the Hessian matrix but also a higher computation cost per step.
            
             Regularization is a method that can render an ill-posed problem more tractable by imposing constraints that provide information to supplement the data and that prevents overfitting by penalizing model's magnitude usually measured by some norm functions.
             This can improve the generalization of the model learned by selecting the optimal complexity in the bias-variance trade-off.
             Regularization works by adding the penalty that is associated with coefficient values to the error of the hypothesis.
             An accurate model with extreme coefficient values would be penalized more, but a less accurate model with more conservative values would be penalized less.
            
             This learner supports [elastic net regularization](https://en.wikipedia.org/wiki/Elastic_net_regularization): a linear combination of L1-norm (LASSO), $|| \boldsymbol{w} ||_1$, and L2-norm (ridge), $|| \boldsymbol{w} ||_2^2$ regularizations.
             L1-norm and L2-norm regularizations have different effects and uses that are complementary in certain respects.
             Using L1-norm can increase sparsity of the trained $\boldsymbol{w}$.
             When working with high-dimensional data, it shrinks small weights of irrelevant features to 0 and therefore no reource will be spent on those bad features when making prediction.
             If L1-norm regularization is used, the used training algorithm would be [QWL-QN](http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.5260).
             L2-norm regularization is preferable for data that is not sparse and it largely penalizes the existence of large weights.
            
             An aggressive regularization (that is, assigning large coefficients to L1-norm or L2-norm regularization terms) can harm predictive capacity by excluding important variables out of the model.
             Therefore, choosing the right regularization coefficients is important when applying maximum entropy classifier.
             ]]></format>
    </remarks>
    <altmember cref="M:Microsoft.ML.StandardTrainersCatalog.LbfgsMaximumEntropy(Microsoft.ML.MulticlassClassificationCatalog.MulticlassClassificationTrainers,System.String,System.String,System.String,System.Single,System.Single,System.Single,System.Int32,System.Boolean)" />
    <altmember cref="M:Microsoft.ML.StandardTrainersCatalog.LbfgsMaximumEntropy(Microsoft.ML.MulticlassClassificationCatalog.MulticlassClassificationTrainers,Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer.Options)" />
    <altmember cref="T:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer.Options" />
  </Docs>
  <Members>
    <Member MemberName="Fit">
      <MemberSignature Language="C#" Value="public Microsoft.ML.Data.MulticlassPredictionTransformer&lt;Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt; Fit (Microsoft.ML.IDataView trainData, Microsoft.ML.Trainers.MaximumEntropyModelParameters modelParameters);" />
      <MemberSignature Language="ILAsm" Value=".method public hidebysig instance class Microsoft.ML.Data.MulticlassPredictionTransformer`1&lt;class Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt; Fit(class Microsoft.ML.IDataView trainData, class Microsoft.ML.Trainers.MaximumEntropyModelParameters modelParameters) cil managed" />
      <MemberSignature Language="DocId" Value="M:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer.Fit(Microsoft.ML.IDataView,Microsoft.ML.Trainers.MaximumEntropyModelParameters)" />
      <MemberSignature Language="VB.NET" Value="Public Function Fit (trainData As IDataView, modelParameters As MaximumEntropyModelParameters) As MulticlassPredictionTransformer(Of MaximumEntropyModelParameters)" />
      <MemberSignature Language="F#" Value="override this.Fit : Microsoft.ML.IDataView * Microsoft.ML.Trainers.MaximumEntropyModelParameters -&gt; Microsoft.ML.Data.MulticlassPredictionTransformer&lt;Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;" Usage="lbfgsMaximumEntropyMulticlassTrainer.Fit (trainData, modelParameters)" />
      <MemberType>Method</MemberType>
      <AssemblyInfo>
        <AssemblyName>Microsoft.ML.StandardTrainers</AssemblyName>
        <AssemblyVersion>1.0.0.0</AssemblyVersion>
      </AssemblyInfo>
      <ReturnValue>
        <ReturnType>Microsoft.ML.Data.MulticlassPredictionTransformer&lt;Microsoft.ML.Trainers.MaximumEntropyModelParameters&gt;</ReturnType>
      </ReturnValue>
      <Parameters>
        <Parameter Name="trainData" Type="Microsoft.ML.IDataView" />
        <Parameter Name="modelParameters" Type="Microsoft.ML.Trainers.MaximumEntropyModelParameters" />
      </Parameters>
      <Docs>
        <param name="trainData">To be added.</param>
        <param name="modelParameters">To be added.</param>
        <summary>
            Continues the training of a <see cref="T:Microsoft.ML.Trainers.LbfgsMaximumEntropyMulticlassTrainer" /> using an already trained <paramref name="modelParameters" /> and returns
            a <see cref="T:Microsoft.ML.Data.MulticlassPredictionTransformer`1" />.
            </summary>
        <returns>To be added.</returns>
        <remarks>To be added.</remarks>
      </Docs>
    </Member>
  </Members>
</Type>
