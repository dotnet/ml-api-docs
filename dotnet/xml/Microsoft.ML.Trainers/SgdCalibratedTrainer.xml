<Type Name="SgdCalibratedTrainer" FullName="Microsoft.ML.Trainers.SgdCalibratedTrainer">
  <TypeSignature Language="C#" Value="public sealed class SgdCalibratedTrainer : Microsoft.ML.Trainers.SgdBinaryTrainerBase&lt;Microsoft.ML.Calibrators.CalibratedModelParametersBase&lt;Microsoft.ML.Trainers.LinearBinaryModelParameters,Microsoft.ML.Calibrators.PlattCalibrator&gt;&gt;" />
  <TypeSignature Language="ILAsm" Value=".class public auto ansi sealed beforefieldinit SgdCalibratedTrainer extends Microsoft.ML.Trainers.SgdBinaryTrainerBase`1&lt;class Microsoft.ML.Calibrators.CalibratedModelParametersBase`2&lt;class Microsoft.ML.Trainers.LinearBinaryModelParameters, class Microsoft.ML.Calibrators.PlattCalibrator&gt;&gt;" />
  <TypeSignature Language="DocId" Value="T:Microsoft.ML.Trainers.SgdCalibratedTrainer" />
  <TypeSignature Language="VB.NET" Value="Public NotInheritable Class SgdCalibratedTrainer&#xA;Inherits SgdBinaryTrainerBase(Of CalibratedModelParametersBase(Of LinearBinaryModelParameters, PlattCalibrator))" />
  <TypeSignature Language="F#" Value="type SgdCalibratedTrainer = class&#xA;    inherit SgdBinaryTrainerBase&lt;CalibratedModelParametersBase&lt;LinearBinaryModelParameters, PlattCalibrator&gt;&gt;" />
  <AssemblyInfo>
    <AssemblyName>Microsoft.ML.StandardTrainers</AssemblyName>
    <AssemblyVersion>1.0.0.0</AssemblyVersion>
  </AssemblyInfo>
  <Base>
    <BaseTypeName>Microsoft.ML.Trainers.SgdBinaryTrainerBase&lt;Microsoft.ML.Calibrators.CalibratedModelParametersBase&lt;Microsoft.ML.Trainers.LinearBinaryModelParameters,Microsoft.ML.Calibrators.PlattCalibrator&gt;&gt;</BaseTypeName>
    <BaseTypeArguments>
      <BaseTypeArgument TypeParamName="TModel">Microsoft.ML.Calibrators.CalibratedModelParametersBase&lt;Microsoft.ML.Trainers.LinearBinaryModelParameters,Microsoft.ML.Calibrators.PlattCalibrator&gt;</BaseTypeArgument>
    </BaseTypeArguments>
  </Base>
  <Interfaces />
  <Docs>
    <summary>
            The <see cref="T:Microsoft.ML.IEstimator`1" /> for training logistic regression using a parallel stochastic gradient method.
            The trained model is <a href="tmpurl_calib">calibrated</a> and can produce probability by feeding the output value of the
            linear function to a <see cref="T:Microsoft.ML.Calibrators.PlattCalibrator" />.
            </summary>
    <remarks>
            The Stochastic Gradient Descent (SGD) is one of the popular stochastic optimization procedures that can be integrated
            into several machine learning tasks to achieve state-of-the-art performance. This trainer implements the Hogwild SGD for binary classification
            that supports multi-threading without any locking. If the associated optimization problem is sparse, Hogwild SGD achieves a nearly optimal
            rate of convergence. For more details about Hogwild SGD, please refer to http://arxiv.org/pdf/1106.5730v2.pdf.
            </remarks>
  </Docs>
  <Members />
</Type>